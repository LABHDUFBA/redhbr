--- 
title: | 
      ![](./img/logo.jpg){width=100%}
author:
- Leonardo F. Nascimento^[UFBA/ICTI/LABHDUFBA/PPGCS, leofn@ufba.br]
- Eric Brasil^[UNILAB - LABHDUFBA, profericbrasil@unilab.edu.br]
- Gabriel Andrade^[LABHDUFBA, gabriel.andrad4@gmail.com] 
- Tarssio Barreto^[LABHDUFBA, tarssioesa@gmail.com ]
- Vítor Mussa^[UFRJ/PPGSA/DTA - LABHDUFBA, vtrmussa@gmail.com]
- Daniel Mendes^[UFRJ/PATHS, daniel_mnds34@hotmail.com]
date: "`r Sys.Date()`"
url: 'https://labhdufba.github.io/redhbr/'
github-repo: "LABHDUFBA/redhbr/"
cover-image: "img/logo.jpg"
favicon: "img/favicon.ico"
output: pdf_document
documentclass: book
bibliography:
- book.bib
- packages.bib
biblio-style: apalike
link-citations: yes
description: Obra colaborativa com scripts e dados da produção intelectual brasileira em todos os campos das humanidades coletadas e agrupadas através de técnicas computacionais de extração autoamtizada (web scraping)
site: bookdown::bookdown_site
header-includes:
  - \usepackage{titling}
  - \pretitle{\begin{center}
    \includegraphics[width=2in,height=2in]{logo.jpg}\LARGE\\}
  - \posttitle{\end{center}}
---
```{js, echo = FALSE}
title=document.getElementById('header');
title.innerHTML = '<img src="./img/logo.jpg" alt="Test Image">' + title.innerHTML
```
---

# Apresentação

<style>
body {
text-align: justify}
</style>

A ideia desta obra foi reunir esforços de diferentes pesquisadores e instituições na elaboração de scripts para coletar - de modo automatizado - a produção intelectual dos principais congressos e eventos das áreas das humanidades. 

Além disso, nós tivemos como objetivo mais amplo enfatizar a importância do desenvolvimento de habilidades computacionais por parte dos pesquisadores em todos os campos das humanidades.

Os scripts, as bases de dados e todos os documentos estão disponíveis e poderão ser baixados com apenas um clique. O acervo servirá para a realização de investigações sobre os mais variados aspectos e ampliar, com isso, o conhecimento sobre a produção acadêmica, científica e intelectual do Brasil das ciências humanas e sociais ao longo de décadas. 

Para o lancamento, nós escolhemos o [Dia Internacional das Humanidades Digitais](https://dhcenternet.org/initiatives/day-of-dh/2021) em 29/04/2021.

Ao compartilhar nas redes, pedimos que usem a hashtag **#dayofdh21**

<center>

![Símbolo do #dayofdh21](./img/dayofdh.jpg){width=35%}

</center>


# Web scraping e ciências sociais

## Por que automatizar?

A dataficação e a digitalização tornaram-se fenômenos massivos das sociedades contemporâneas. Ao interargirmos com as tecnologias digitais nós deixamos traços de dados que podem ser usados para a pesquisa sobre a sociedade. O desafio colocado para os pesquisadores das humanidades está em acessar e manipular tais dados:

> “Como uma técnica de extração de dados online, o [web scraping] parece de interesse especial para nós porque é uma parte importante do que torna a pesquisa social digital praticamente possível.” [(MARRES, N. & WELTEVREDE, E. Scraping the Social? Journal of Cultural Economy, v. 6, n. 3, p. 313–335, 1 ago. 2013, p.317)](https://www.tandfonline.com/doi/abs/10.1080/17530350.2013.772070)

O volume, quantidade e qualidade dos dados digitais e digitalizados nunca foi tão grande. O acesso à fontes digitalizadas através de mecanismos de busca por palavras-chave, por assuntos, por metadatos em geral, os milhares de dados produzidos a cada segundo nas redes sociais ou o volume de publicações acadêmicas têm impactado as pesquisas e a própria construção do conhecimento nas ciências humanas e sociais.

Assim, é urgente a necessidade de enfrentarmos os desafios metodológicos e teóricos colocados por esse cenário. A automatização na coleta de dados na Web não é apenas uma forma de acelarar essa relação do pequisador com os dados, mas de qualificar e potencializar a tarefa heurística de seleção dos mesmos. 

## Como começar?

É preciso aprender algum tipo de linguagem de programação (geralmente R ou Python), além de conhecimentos em HTML, CSS e XPATH. Sabemos que, à primeira vista, parecem ser termos complicados para quem vem "das humanas", mas o entendimento destas coisas é relativamente mais simples que muitas das leituras que nós fazemos. 

Portanto, talvez o primeiro passo seja buscar compreender a estrutura da página que abriga os dados que você pretende coletar. Para isso, é preciso conhecer o mínimo de HTML.

Em seguida é importante definir quais dados e informações você pretende coletar e qual a estrtura de organização você pretende construir como resultado. Esse é um procedimento metodológico fundamental para a pesquisa e demanda do pesquisador o mesmo rigor acadêmico do trabalho com dados de outra natureza.

Por fim, a escrita do código, utilizando a linguagem que melhor atenda aos seus interesses.

Todos esses processos demandam um empenho de tempo e formação técnicas específicas, sem dúvida. Entretanto, acreditamos que os retornos possíveis justificam o investimento de tempo. Além disso, amplia as possibilidades de trabalho interdisciplinar, colaborativo e aberto.

## Web scraping enquanto técnica das humanidades

Ao realizarmos um web scraping é preciso atentar para os procedimentos não apenas “técnicos” envolvidos na raspagem mas, também, para os aspectos analíticos e epistemológicos. Cada plataforma, website ou [API](https://canaltech.com.br/software/o-que-e-api/) possui características particulares que vão, juntamente com o código que vamos contruir, determinar o tipo e natureza dos dados coletados.

> A raspagem, entretanto, não é apenas uma técnica, mas também envolve uma forma particular de lidar com a informação e o conhecimento: é também uma prática analítica.[(MARRES, N. & WELTEVREDE, E. Scraping the Social? Journal of Cultural Economy, v. 6, n. 3, p. 313–335, 1 ago. 2013, p.317)](https://www.tandfonline.com/doi/abs/10.1080/17530350.2013.772070)

Erros no código de raspagem podem produzir dados distorcidos, com lacunas ou mesmo em duplicidade. Podemos, então, considerar que um erro no código torna-se um erro metodológico.

# Linguagens de programação

## R

R é uma linguagem e um ambiente de desenvolvimento integrado, para cálculos estatísticos e gráficos. Foi criada originalmente por Ross Ihaka e por Robert Gentleman no departamento de Estatística da universidade de Auckland, Nova Zelândia, e foi desenvolvido por um esforço colaborativo de pessoas em vários locais do mundo.O nome R provêm em parte das iniciais dos criadores e também de um jogo figurado com a linguagem S (da Bell Laboratories, antiga AT&T).

O código fonte do R está disponível sob a licença GNU4 5 GPL e as versões binárias pré-compiladas são fornecidas para Windows, Macintosh, e muitos sistemas operacionais Unix/Linux. R é também altamente expansível com o uso dos pacotes, que são bibliotecas para funções específicas ou áreas de estudo específicas.Um conjunto de pacotes é incluído com a instalação de R, com muito outros disponíveis na rede de distribuição do R (em inglês CRAN).

Um dos pontos fortes da linguagem R é a sua comunidade, extremamente acolhedora e que estimula todos os novos participantes. No Telegram existe existe um grupo extremamente ativo onde é possível tirar dúvidas: [R Brasil](https://t.me/rbrasiloficial)



## Python

Alguns dos códigos que compõe o Redhbr foram escritos em [Python 3.8](https://www.python.org/). Esta é uma linguagem de programação que permite ao programados trabalhar rapidamente e integrar diferentes sistemas com maior eficiência.

>Foi lançada por Guido van Rossum em 1991. Atualmente, possui um modelo de desenvolvimento comunitário, aberto e gerenciado pela organização sem fins lucrativos [Python Software Foundation](https://www.python.org/psf/).^[[Python - Wikipedia.org](https://pt.wikipedia.org/wiki/Python)]

Parte da filosofia da linguagem está resumida no poema _Zen of Python_, [escrito por Tim Peters em 1999](https://pt.wikipedia.org/wiki/Python).

>Bonito é melhor que feio
>
>Explícito é melhor que implícito
>
>Simples é melhor que complexo
>
>Complexo é melhor que complicado
>
>Linear é melhor do que aninhado
>
>Esparso é melhor que denso
>
>Legibilidade conta
>
>Casos especiais não são especiais o bastante para quebrar as regras.
>
>Ainda que praticidade vença a pureza
>
>Erros nunca devem passar silenciosamente. 
>
>A menos que sejam explicitamente silenciados
>
>Diante da ambiguidade, recuse a tentação de adivinhar
>
>Deveria haver um — e preferencialmente apenas um — modo óbvio para fazer algo.
>
>Embora esse modo possa não ser óbvio a princípio a menos que você seja holandês
>
>Agora é melhor que nunca
>
>Embora nunca freqüentemente seja melhor que já
>
>Se a implementação é difícil de explicar, é uma má ideia
>
>Se a implementação é fácil de explicar, pode ser uma boa ideia
>
>Namespaces são uma grande ideia — vamos ter mais dessas!^[[Zen of Python - Wikipedia.org](https://pt.wikipedia.org/wiki/Zen_of_Python)]

Para executar um arquivo .py é preciso instalar o Python3 em seu computador.

[Clique aqui](https://python.org.br/instalacao-windows/) para um tutorial de instalação do Python no Windows, [clique aqui](https://python.org.br/instalacao-linux/) para Linux e [clique aqui](https://python.org.br/instalacao-mac/)
para Mac.

Após a instalação, vc pode executar o arquivo .py direto do prompt de comando do Windows ou pelo terminal do Linux, ou utilizar as diversas [IDE](https://pt.wikipedia.org/wiki/Ambiente_de_desenvolvimento_integrado) disponíveis.

Segue um exemplo de como executar utilizando o terminal do Linux, após instalar o Python3.8:

1. Acesse o diretório em que o arquivo .py está salvo:
   
sh
   ```$ cd "caminho do diretório"```
   
1. Instale as bibliotecas requeridas:
   
sh
   ```$ pip3 install -r requirements.txt```
   
1. Execute o arquivo usando Python3.8
   
sh
   ```$ python3 script-anais-anpuh.py```


# ANPUH

## O que é ANPUH?

A [Associação Nacional de História, Anpuh](https://anpuh.org.br/index.php), fundada em 1961, inicialmente destinada aos docentes de cursos de graduação e pós-graduação. Em 1993, a ANPUH ampliou sua base para todoa os profissionais de história.

>A cada dois anos, a ANPUH realiza o Simpósio Nacional de História, o maior e mais importante evento da área de história no país e na América Latina^[[Anpuh-Quem somos](https://anpuh.org.br/index.php/quem-somos)].

Desenvolvemos scripts diferentes para dois tipos de conjuntos de dados relacionados à Associação Nacional de História.

- Anais-Anpuh: script para raspagem de todos os trabalhos publicados nos Anais dos Simpósio Nacionais de História entre 1963 e 2017, disponíveis no site da Anpuh.

- anpuh-scraper: script para raspagem dos resumos (e demais informações) de todos os trabalhos aprovados para todos os simpósios temáticos dos SNH nos aos de 2013, 2015, 2017 e 2019.

## Scripts de raspagem 

### Anais em pdf da ANPUH

[**Clique aqui para acessar o repositório no Github**](https://github.com/LABHDUFBA/Anais-Anpuh)

Esse script realiza a raspagem dos trabalhos em PDF de todos os Simpósios Nacionais da Anpuh entre 1963 até 2017, disponíveis atualmente na site da associação, que podem ser [acessados aqui](https://anpuh.org.br/index.php/documentos/anais).

Escrito em [Python 3.8](https://www.python.org/), o script utiliza as seguintes bibliotecas e módulos

- **urllib.requests**: módulo do Python para acessar urls. [Saiba mais.](https://docs.python.org/pt-br/3/library/urllib.request.htmll)
- **os**: módulo do Python que permite manipular funções do sistema operacional. [Saiba mais.](https://docs.python.org/pt-br/3/library/os.html)
- **bs4**: [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) é uma biblioteca Python para extrair  dados de arquivos HTML e XML.
- **re**: [Regular Expressions](https://docs.python.org/pt-br/3/library/re.html) é um módulo do Python para operar com expressões regulares.
- **pandas**: [Pandas](https://pandas.pydata.org/) é uma biblioteca escrita em Python para manipulação e análise de dados.
- **wget**: [Wget](https://pypi.org/project/wget/) é uma biblioteca escrita em Python para realizar downloads. 

O script tem o seguinte funcionamento quando executado:

- Cria pasta para salvar os PDFs, após verificar se a mesma não existe no local: `Anais Anpuh> pdf` utilizando módulo `os`.
- Acessa a URL dos Anais com a biblioteca `urllib` e realiza a análise do HTML da mesma com a biblioteca `BeautifulSoup`;
- Cria uma lista de eventos a partir da página principal;
- Acessa as páginas de cada evento contidas na lista criada anteriormente através de uma iteração;
- Em cada item da lista de eventos, o script busca todos os papers da primeira página e cria uma nova lista. Nessa lista de papers de uma dada página o script realizará as seguintes ações:
	- encontrar as informações de cada paper;
	- inclui essas informações em uma lista (que depois gerará um CSV com os dados);
	- busca se há pdf disponível e se ele não é repetido faz download do PDF
	- Após realizar essas ações para todos os itens de uma página, busca a próxima página de papers do evento, se não houver, passa para o próximo evento e repete as ações em um _loop_ até o último evento disponível.

### Dados

O script retorna para o usuário **todos os pdfs disponíveis em todas as páginas de todos os Simpósios Nacionais da Anpuh desde 1963 até 2017**. São criadas pastas com o número de cada evento para o armazenamento dos arquivos em PDF.

![](img/pastas.png)

É importante notar que muitos papers não estão com pdf disponível no site, assim como nas edições mais antigas encontramos arquivos que contém vários papers num único PDF.

O script também gera um arquivo **CSV** (*comma-separated values*) contendo os seguintes valores para cada paper: Autor(es)/Instituições,Título, Tipo, Evento, Ano, Link do Arquivo. Esse arquivo pode ser aberto como uma planilha e trabalhado em banco de dados.

![](img/ex_csv1.png)

**Download**: [Se preferir baixar a base dos PDFs sem usar o código clique aqui](https://filesender.rnp.br/?s=download&token=9c7d390c-0033-4777-8e63-31044168c4b1).

### Resumos dos trabalhos da ANPUH

[**Clique aqui para acessar o repositório no Github**](https://github.com/LABHDUFBA/anpuh-scraper).

*Raspador dos resumos dos Simpósios Nacionais de História da [Associação Nacional de História - Anpuh](https://anpuh.org.br). O programa raspa todos os resumos dos SNH 27, 28, 29 e 30, respectivamente dos anos de 2013, 2015, 2017 e 2019*
Escrito em [Python 3.8](https://www.python.org/), o script utiliza as seguintes bibliotecas e módulos

- **urllib.requests**: módulo do Python que ajuda a acessar urls.
[Saiba mais.](https://docs.python.org/pt-br/3/library/urllib.request.htmll)
- **bs4**: [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) é uma biblioteca Python para extrair  dados de arquivos HTML e XML.
- **pandas**: [Pandas](https://pandas.pydata.org/) é uma biblioteca escrita em Python para manipulação e análise de dados. 

O script tem o seguinte funcionamento quando executado:

Pergunta ao usuário que ano pretende raspar e se deseja incluir um novo ano à lista. 
Após a criação da lista com os anos escolhidos pelo usuário, o script acessa cada uma das páginas com as listas dos STs nos sites de cada evento;
Acessa cada ST, encontra os dados de todos os resumos e passa para o ST seguinte;
Após terminar um ST, passa para o próximo evento e executa as mesmas função;
Todos os dados são inseridos em um DataFrame em Pandas e ao final são salvos no formato CSV.

### Dados

O script retorna para o usuário um **CSV (*comma-separated values*) com os dados de todos os trabalhos aceitos nos Simpósio Temáticos dos SNH 27, 28, 29 e 30**.

O CSV contém as seguintes variáveis para cada resumo:

`Ano, Evento, Cidade, ST, Coordenadores, Autor(es)/Instituições, Título, Resumo`

Esse arquivo pode ser aberto como uma planilha e trabalhado em banco de dados.

![](img/ex_csv2.png)

**Download**: [Se preferir baixar a planilha sem usar o código clique aqui](https://drive.google.com/file/d/12QCIsvFGIhveD84HUKwsLG-RbtpKBNsh/view?usp=sharing).

# ANPOCS

## O que é a ANPOCS?

A [Associação Nacional de Pós-Graduação e Pesquisa em Ciências Sociais (ANPOCS)](http://anpocs.com/) é uma entidade de direito privado sem fins lucrativos que reúne centenas de centros de pós-graduação e de pesquisa em antropologia, ciência política, relações internacionais e sociologia de todo o Brasil. Ela é formada, portanto, por instituições, em vez de pesquisadores individuais.

A associação organiza os _Encontros Anuais da ANPOCS_, que consistem em congressos cujo número médio de participantes é de 1500 pesquisadores. Esses encontros estão entre os fóruns mais relevantes para as ciências sociais no Brasil.

Diante disso, desenvolvemos o `anpocs-scraper` – [disponível aqui](https://github.com/vmussa/anpocs-scraper) –, um raspador que permite coletar de forma automatizada os dados dos resumos dos trabalhos apresentados nos encontros de 2019, 2020 e, futuramente, 2021. O raspador expressa mais uma iniciativa que busca contribuir para uma ciência aberta e transparente, facilitando o acesso aos dados dos congressos e contribuindo para a preservação da memória das ciências sociais brasileiras.

## Script de raspagem

### anpocs-scraper

![](img/demo.gif "Demonstração do anpocs-scraper.")

O `anpocs-scraper` é um raspador dos dados dos [Encontros Anuais da ANPOCS](http://anpocs.com/index.php/encontros/apresentacao) escrito em Python. Atualmente o código permite coletar:

* os dados de todos os resumos dos trabalhos apresentados em GT's e SPG's do [44º Encontro Anual da ANPOCS](https://www.anpocs2020.sinteseeventos.com.br/)

* os dados de todos os resumos dos trabalhos apresentados em ST's e SPG's do [43º Encontro Anual da ANPOCS](http://anpocs.com/index.php/43-encontro-anual-2019/2750-encontros-anuais/43-encontro/2301-resumos-sts-e-spgs)

### Instalação e modo de uso

Para instalar o raspador basta clonar o repositório, [que se encontra aqui](https://github.com/vmussa/anpocs-scraper), e instalar suas dependências:

```
git clone https://github.com/vmussa/anpocs-scraper
cd anpocs-scraper
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
```

Para rodar o raspador, continue no repositório clonado e execute o código `main.py` com o Python:

```
python src/main.py
```

> Atenção &#8594; **para realizar esse procedimento, você precisa instalar o Google Chrome e o ChromeDriver**: [Clique aqui](https://chromedriver.chromium.org/getting-started) para ler um tutorial sobre como instalar o ChromeDriver.


## Dados

O programa exporta, para cada edição do congresso, uma tabela no formato CSV com as seguintes informações de cada trabalho apresentado:

> `autores`, `titulo`, `resumo`, `sessao`, `id_evento`

A imagem abaixo ilustra o formato de uma das tabelas:

![](img\anpocs_scraper_csv.png)

Caso você não queira rodar o raspador, mas precise dos dados, você pode obter [a versão atual do conjunto de dados exportado pelo programa aqui](https://drive.google.com/drive/folders/1XdHLf3b7r0d1EwwlYo4D0bRBujVILBq7?usp=sharing).

## Em breve

Futuramente o raspador abarcará todos os GT’s e SPG’s do encontro 45, cujos resumos dos trabalhos estarão disponíveis aqui. Além disso, ele contará com um módulo de limpeza dos dados que fará o pré-processamento para a análise qualitativa e/ou computacional. Por fim, disponibilizaremos, também, todos os PDFs de artigos enviados para todas as edições dos Encontros Anuais da ANPOCS.


# COMPÓS

## O que é a COMPÓS?

[A COMPÓS - Associação Nacional dos Programas de Pós-Graduação em Comunicação - foi fundada em 16 junho de 1991, em Belo Horizonte, com o apoio da Capes e do CNPq, a partir da iniciativa de alguns pesquisadores e representantes dos seguintes cursos de Pós-Graduação: PUC-SP, UFBA, UFRJ, UnB, UNICAMP, UMESP. É uma sociedade civil, sem fins lucrativos, congregando como associados os Programas de Pós-Graduação em Comunicação em nível de Mestrado e/ou Doutorado de instituições de ensino superior públicas e privadas no Brasil. A COMPÓS tem como objetivos principais o fortalecimento e qualificação crescentes da Pós-Graduação em Comunicação no país; a integração e intercâmbio entre os Programas existentes, bem como o apoio à implantação de novos Programas; o diálogo com instituições afins nacionais e internacionais; o estímulo à participação da comunidade acadêmica em Comunicação nas políticas do país para a área, defendendo o aperfeiçoamento profissional e o desenvolvimento teórico, cultural, científico e tecnológico no campo da Comunicação.](https://www.compos.org.br/a_compos.php)

[Como espaço de intercâmbio acadêmico entre os pesquisadores dos vários Programas, a COMPÓS tem como fórum privilegiado os Encontros Anuais, estruturados sob a forma de Grupos de Trabalhos (GTs), onde são apresentados e debatidos estudos que buscam refletir sobre o avanço científico, tecnológico e cultural no campo da comunicação](https://www.compos.org.br/encontros_anuais.php)

Diante disso, desenvolvemos o `Anais-COMPOS-scraper` – [disponível aqui](https://github.com/LABHDUFBA/Anais-COMPOS-scraper) – que realiza o download automatizado de todos os papers em pdf dos Encontros da COMPÓS entre 2000 até 2020 (disponíveis atualmente na site). Além disso, o script também gera um arquivo CSV (comma-separated values) contendo as seguintes informações para cada paper: Ano, Edição, Nome do GT, Título, Autores, e Link do Arquivo. Esse arquivo pode ser aberto como uma planilha e trabalhado em banco de dados.

O raspador expressa mais uma iniciativa que busca contribuir para uma ciência aberta e transparente, facilitando o acesso aos dados dos congressos e contribuindo para a preservação da memória das ciências sociais brasileiras.


## Script de raspagem

### R e RStudio

O R e RStudio são gratuitos e possuem versões para Windows, Mac e Linux. A instalação é bastante fácil e em geral você apenas tem que seguir as instruções da tela.

Para instalar o R, baixe a versão adequada para seu computador em: [https://cloud.r-project.org/](https://cloud.r-project.org/)

Para instalar o RStudio, baixe a versão adequada para seu computador em: [https://www.rstudio.com/products/rstudio/download/](https://www.rstudio.com/products/rstudio/download/)

Além disso, para ter um ambiente completo de desenvolvimento no R, recomendamos, adicionalmente, instalar:

– MikTex (para Windows:  [http://miktex.org/download](http://miktex.org/download) ou MacTex (para Mac:  [https://tug.org/mactex/downloading.html](https://tug.org/mactex/downloading.html) para relatórios em latex.

– RTools (para Windows: [https://cran.r-project.org/bin/windows/Rtools/](https://cran.r-project.org/bin/windows/Rtools/) ou Xcode com command line tools (para Mac na AppStore do Mac), para criar pacotes, usar C++ com R entre outras  coisas

Após a instalação, vc pode executar o arquivo **compos.R** que está na pasta **R** direto do RStudio.


### Bibliotecas e módulos

Vocêr vai precisar instalar as seguintes bibliotecas: 

1. [RSelenium](https://cran.r-project.org/web/packages/RSelenium/RSelenium.pdf)
2. [tidyverse](https://www.tidyverse.org/)
3. [rvest](https://cran.r-project.org/web/packages/rvest/rvest.pdf)

### Chromedriver

1. [Instruções sobre como instalar o Chromedriver no Windows 10 :](https://www.youtube.com/watch?v=dz59GsdvUF8) 

2. [Instruções sobre como instalar o Chromedriver no Ubuntu :](https://medium.com/@marco.conviccao/configurando-o-chromedriver-no-ubuntu-7baaf2be7c68)


## Dados

O programa exporta, para cada edição do congresso, uma tabela no formato CSV com as seguintes informações de cada trabalho apresentado:

> `Ano`, `Edição`, `Nome do GT`, `Título`, `Autores`, `Links`.  

A imagem abaixo ilustra o formato de uma das tabelas:

![](img\csv.png)

**Download**: [Se preferir baixar a base dos PDFs sem usar o código clique aqui.](https://filesender.rnp.br/?s=download&token=ae3e90f6-c8db-497f-a3e0-73958c634997)
(**OBS:a nomeação ainda contém alguns pequenos erros que serão corrigidos em breve**)

**Download**: [Se preferir baixar a planilha sem usar o código clique aqui.](https://docs.google.com/spreadsheets/d/1VXlCD9rak-ut3_5p6UFyzOfuSgpbBcl9KDHrXe53q-0/edit?usp=sharing)

# Sobre os autores

<style>
body {
text-align: justify}
</style>


## Leonardo F. Nascimento (UFBA/ICTI/LABHDUFBA/PPGCS)

Sou técnico em Química pelo Instituto Federal de Educação, Ciência e Tecnologia da Bahia – IFBA (1997), graduado em Psicologia pela Universidade Federal da Bahia – UFBA (2002), mestre em Sociologia pela Universidade de São Paulo – USP (2007) e doutor em Sociologia pelo Instituto de Estudos Sociais e Políticos – IESP/UERJ (2013). Entre 2010 e 2011, realizei estágio doutoral na *École des Hautes Études en Sciences Sociales* (EHESS). 

Nos últimos anos estou totalmente dedicado ao estudo das novas tecnologias aplicadas à pesquisa e análise de dados em Ciências Sociais, especialmente com uso de CAQDAS (Computer Assisted/Aided Qualitative Data Analysis). Eu sou professor do Bacharelado Interdisciplinar (BI) em Ciência, Tecnologia e Inovação da UFBA e membro permanente do Programa de Pós-Graduação em Ciências Sociais da UFBA (PPGCS/UFBA). Eu desenvolvo pesquisas sobre sociologia digital, mineração de dados, ciência social computacional e análise de mídia. Em 2018 eu ajudei a criar o Laboratório de Humanidades Digitais da UFBA, uma convergência de pesquisadores e interesses de pesquisa em torno dos temas da ciência social computacional,
humanidades e métodos digitais.

Currículos e redes acadêmicas

[Webpage](https://leofn.com/) - 
[Lattes](http://lattes.cnpq.br/7141811368487014) - [LinkedIn](https://www.linkedin.com/in/leonardo-nascimento-labhdufba/) - [Twitter](https://twitter.com/leofn3) -
[Orcid](https://orcid.org/0000-0003-2929-1115) - [ResearchGate](https://www.researchgate.net/profile/Leonardo-Nascimento-2)

 
## Eric Brasil (IHLM/UNILAB)

Professor do curso de licenciatura em História e professor do Bacharelado Interdisciplinar em Humanidades no Instituto de Humanidades e Letras da Universidade da Integração Internacional da Lusofonia Afro-brasileira (IHL-UNILAB), campus dos Malês, Bahia. Doutor (2016) e Mestre (2011) pelo Programa de Pós-Graduação em História Social da Universidade Federal Fluminense. Autor do livro A Corte em Festa: experiências negras em carnavais do Rio de Janeiro (1879-1888) (Editora Prismas, 2016). Vencedor do primeiro lugar no Concurso de Monografias Silvio Romero de 2011 e do segundo lugar em 2020, promovido pelo Centro Nacional de Folclore e Cultura Popular.Foi professor de ensino fundamental, médio e pré-vestibular no Rio de Janeiro entre 2007 e 2017.

Pesquisador do Laboratório de Humanidades Digitais da UFBA. Membro do GT Nacional Emancipações e Pós-Abolição da Anpuh. Tem experiência na área de História Social da Cultura, Humanidades e História Digital, Abolição da escravidão e o Pós-Abolição no Brasil e no Caribe, atuando principalmente nos seguintes temas: Carnaval, Cidadania, História Transnacional, Diáspora Africana, História das Afro-Américas, Hemerotecas e arquivos digitais, métodos digitais de pesquisa, linguagem de programação para a pesquisa em História, web scraping. 


Currículos e redes acadêmicas

[Webpage](https://ericbrasiln.github.io) - [Lattes](http://lattes.cnpq.br/6853705640900524) - [Orcid]("https://orcid.org/0000-0001-5067-8475) - [ResearchGate](https://www.researchgate.net/profile/Eric_Brasil) - [Academia.edu](https://unilab.academia.edu/EricBrasil)

## Vítor Mussa (DTA/PPGSA/UFRJ e LABHDUFBA/UFBA)

Mestrando do Programa de Pós-Graduação em Sociologia e Antropologia (PPGSA) da Universidade Federal do Rio de Janeiro (UFRJ).

É membro do grupo de pesquisa Desenvolvimento, Trabalho e Ambiente ([DTA-UFRJ](https://www.nucleodta.org/inicio)) e do Laboratório de Humanidades Digitais da Universidade Federal da Bahia ([LABHD-UFBA](http://www.labhd.ufba.br/)).

Currículos e redes acadêmicas

[Webpage](https://vmussa.github.io) - [Lattes](http://lattes.cnpq.br/2934187748254130) - [ResearchGate](https://www.researchgate.net/profile/Vitor-Mussa-2) - [LinkedIn](https://www.linkedin.com/in/vmussa/) - [Twitter](https://twitter.com/vitormussa)

## Tarssio Barreto (LABHDUFBA)

Co-fundador da Bit Analytics, doutorando do Programa de Pós Graduação em Engenharia Industrial(PEI/UFBA) com foco na área de técnicas de agrupamentos voltadas para Machine Learning. Mestre em Meio Ambiente Águas e Saneamento - UFBA (2018) e graduado em Engenharia Sanitária e Ambiental pela UFBA (2015). Área de atuação: Machine Learning; modelagem probabilística e simulação e programação avançada em R.#RStats

[Lattes](http://lattes.cnpq.br/8314700954142455) - 
[LinkedIn](https://www.linkedin.com/in/tarssio-brito-barreto-9646b817b/) - 
[Twitter](https://twitter.com/danielmnds34)


## Daniel Mendes (PATHS/UFRJ)

Graduando no curso de Bacharelado em Ciências Sociais do Instituto de Filosofia e Ciências Sociais (IFCS) da Universidade Federal do Rio de Janeiro (UFRJ).

É membro do grupo de pesquisa Núcleo de Pesquisa em Estratificação e Trajetórias Sociais ([PATHS](https://www.facebook.com/paths.research/)).

Currículos e redes acadêmicas

[Lattes](http://lattes.cnpq.br/9834413442426550) - [LinkedIn](https://www.linkedin.com/in/daniel-mendes-251212176/) - [Twitter](https://twitter.com/danielmnds34)

## Gabriel Andrade (LABHDUFBA/UFBA)

Graduando no curso de Bacharelado em Engenharia de Computação da Escola Politécnica da Universidade Federal da Bahia (UFBA)

É desenvolvedor de software e membro do Laboratório de Humanidades Digitais da Universidade Federal da Bahia ([LABHDUFBA](http://www.labhd.ufba.br/)).

Currículos e redes acadêmicas

[Webpage](https://gabrielsandrade.github.io) - [Lattes](http://lattes.cnpq.br/4915378425369073) - [LinkedIn](https://www.linkedin.com/in/gabriel-andrade-633996108) - [Twitter](https://twitter.com/ga_brieell_)

## Ajude o projeto! 

<center>

![bc1qmug7kcrw3kxklca7chy7c344d62gtc8fhqwnkw](C:/Users/Leonardo/Desktop/redhbr/img/btc.png){width=35%}

</center>

```{js, echo = FALSE}
title=document.getElementById('header');
title.innerHTML = '<img src="https://labhdufba.github.io/redhbr/img/logo.jpg" alt="Test Image">' + title.innerHTML
```
