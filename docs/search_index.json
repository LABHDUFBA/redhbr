[["index.html", " 1 Apresentação", " Leonardo F. Nascimento1 Eric Brasil2 Gabriel Andrade3 Tarssio Barreto4 Vítor Mussa5 Daniel Mendes6 2021-04-28 1 Apresentação body { text-align: justify} A ideia desta obra foi reunir esforços de diferentes pesquisadores e instituições na elaboração de scripts para coletar - de modo automatizado - a produção intelectual dos principais congressos e eventos das áreas das humanidades. Além disso, nós tivemos como objetivo mais amplo enfatizar a importância do desenvolvimento de habilidades computacionais por parte dos pesquisadores em todos os campos das humanidades. Os scripts, as bases de dados e todos os documentos estão disponíveis e poderão ser baixados com apenas um clique. O acervo servirá para a realização de investigações sobre os mais variados aspectos e ampliar, com isso, o conhecimento sobre a produção acadêmica, científica e intelectual do Brasil das ciências humanas e sociais ao longo de décadas. Para o lancamento, nós escolhemos o Dia Internacional das Humanidades Digitais em 29/04/2021. Ao compartilhar nas redes, pedimos que usem a hashtag #dayofdh21 Símbolo do #dayofdh21 UFBA/ICTI/LABHDUFBA/PPGCS, leofn@ufba.br UNILAB - LABHDUFBA, profericbrasil@unilab.edu.br LABHDUFBA, gabriel.andrad4@gmail.com LABHDUFBA, tarssioesa@gmail.com  UFRJ/PPGSA/DTA - LABHDUFBA, vtrmussa@gmail.com UFRJ/PATHS, daniel_mnds34@hotmail.com "],["webscraping-e-ciências-sociais.html", "2 2 Webscraping e ciências sociais 2.1 2.1 Por que automatizar? 2.2 2.2 Como começar? 2.3 2.3 Webscraping enquanto técnica das humanidades", " 2 2 Webscraping e ciências sociais 2.1 2.1 Por que automatizar? A dataficação e a digitalização tornaram-se fenomenos massivos das sociedades contemporâneas. Ao interargirmos com as tecnologias digitais nós deixamos traços de dados que podem ser usados para a pesquisa sobre a sociedade. O desafio colocado para os pesquisadores das humanidades está em acessar e manipular tais dados: Como uma técnica de extração de dados online, o [webscraping] parece de interesse especial para nós porque é uma parte importante do que torna a pesquisa social digital praticamente possível. (MARRES, N. &amp; WELTEVREDE, E. Scraping the Social? Journal of Cultural Economy, v. 6, n. 3, p. 313335, 1 ago. 2013, p.317) O volume, quantidade e qualidade dos dados digitais e digitalizados nunca foi tão grande. O acesso à fontes digitalizadas através de mecanismos de busca por palavras-chave, por assuntos, por metadatos em geral, os milhares de dados produzidos a cada segundo nas redes sociais ou o volume de publicações acadêmicas têm impactado as pesquisas e a própria construção do conhecimento nas ciências humanas e sociais. Assim, é urgente a necessidade de enfrentarmos os desafios metodológicos e teóricos colocados por esse cenário. A automatização na coleta de dados na Web não é apenas uma forma de acelarar essa relação do pequisador com os dados, mas de qualificar e potencializar a tarefa heurística de seleção dos mesmos. 2.2 2.2 Como começar? É preciso aprender algum tipo de linguagem de programação (geralmente R ou Python), além de conhecimentos em HTML, CSS e XPATH. Sabemos que, à primeira vista, parecem ser termos complicados para quem vem das humanas, mas o entendimento destas coisas é relativamente mais simples que muitas das leituras que nós fazemos. Portanto, talvez o primeiro passo seja buscar compreender a estrutura da página que abriga os dados que você pretende coletar. Para isso, é preciso conhecer o mínimo de HTML. Em seguida é importante definir quais dados e informações você pretende coletar e qual a estrtura de organização você pretende construir como resultado. Esse é um procedimento metodológico fundamental para a pesquisa e demanda do pesquisador o mesmo rigor acadêmico do trabalho com dados de outra natureza. Por fim, a escrita do código, utilizando a linguagem que melhor atenda aos seus interesses. Todos esses processos demandam um empenho de tempo e formação técnicas específicas, sem dúvida. Entretanto, acreditamos que os retornos possíveis justificam o investimento de tempo. Além disso, amplia as possibilidades de trabalho interdisciplinar, colaborativo e aberto. 2.3 2.3 Webscraping enquanto técnica das humanidades Ao realizarmos um webscrapig é preciso atentar para os procedimentos não apenas técnicos envolvidos na raspagem mas, também, para os aspectos analíticos e epistemológicos. Cada plataforma, website ou API possui características particulares que vão, juntamente com o código que vamos contruir, determinar o tipo e natureza dos dados coletados. A raspagem, entretanto, não é apenas uma técnica, mas também envolve uma forma particular de lidar com a informação e o conhecimento: é também uma prática analítica.(MARRES, N. &amp; WELTEVREDE, E. Scraping the Social? Journal of Cultural Economy, v. 6, n. 3, p. 313335, 1 ago. 2013, p.317) Erros no código de raspagem podem produzir dados distorcidos, com lacunas ou mesmo em duplicidade. Podemos, então, considerar que um erro no código torna-se um erro metodológico. "],["linguagens-de-programação.html", "3 Linguagens de programação 3.1 R 3.2 Python", " 3 Linguagens de programação 3.1 R 3.2 Python Alguns dos códigos que compõe o Redhbr foram escritos em Python 3.8. Esta é uma linguagem de programação que permite ao programados trabalhar rapidamente e integrar diferentes sistemas com maior eficiência. Foi lançada por Guido van Rossum em 1991. Atualmente, possui um modelo de desenvolvimento comunitário, aberto e gerenciado pela organização sem fins lucrativos Python Software Foundation.7 Parte da filosofia da linguagem está resumida no poema Zen of Python, escrito por Tim Peters em 1999. Bonito é melhor que feio Explícito é melhor que implícito Simples é melhor que complexo Complexo é melhor que complicado Linear é melhor do que aninhado Esparso é melhor que denso Legibilidade conta Casos especiais não são especiais o bastante para quebrar as regras. Ainda que praticidade vença a pureza Erros nunca devem passar silenciosamente. A menos que sejam explicitamente silenciados Diante da ambiguidade, recuse a tentação de adivinhar Deveria haver um  e preferencialmente apenas um  modo óbvio para fazer algo. Embora esse modo possa não ser óbvio a princípio a menos que você seja holandês Agora é melhor que nunca Embora nunca freqüentemente seja melhor que já Se a implementação é difícil de explicar, é uma má ideia Se a implementação é fácil de explicar, pode ser uma boa ideia Namespaces são uma grande ideia  vamos ter mais dessas!8 Para executar um arquivo .py é preciso instalar o Python3 em seu computador. Clique aqui para um tutorial de instalação do Python no Windows, clique aqui para Linux e clique aqui para Mac. Após a instalação, vc pode executar o arquivo .py direto do prompt de comando do Windows ou pelo terminal do Linux, ou utilizar as diversas IDE disponíveis. Segue um exemplo de como executar utilizando o terminal do Linux, após instalar o Python3.8: Acesse o diretório em que o arquivo .py está salvo: sh $ cd \"caminho do diretório\" Instale as bibliotecas requeridas: sh $ pip3 install -r requirements.txt Execute o arquivo usando Python3.8 sh $ python3 script-anais-anpuh.py Python - Wikipedia.org Zen of Python - Wikipedia.org "],["anpuh.html", "4 ANPUH 4.1 O que é ANPUH? 4.2 Scripts de raspagem", " 4 ANPUH 4.1 O que é ANPUH? A Associação Nacional de História, Anpuh, fundada em 1961, inicialmente destinada aos docentes de cursos de graduação e pós-graduação. Em 1993, a ANPUH ampliou sua base para todoa os profissionais de história. A cada dois anos, a ANPUH realiza o Simpósio Nacional de História, o maior e mais importante evento da área de história no país e na América Latina9. Desenvolvemos scripts diferentes para dois tipos de conjuntos de dados relacionados à Associação Nacional de História. Anais-Anpuh: script para raspagem de todos os trabalhos publicados nos Anais dos Simpósio Nacionais de História entre 1963 e 2017, disponíveis no site da Anpuh. anpuh-scraper: script para raspagem dos resumos (e demais informações) de todos os trabalhos aprovados para todos os simpósios temáticos dos SNH nos aos de 2013, 2015, 2017 e 2019. 4.2 Scripts de raspagem 4.2.1 Anais em pdf da ANPUH Clique aqui para acessar o repositório no Github Esse script realiza a raspagem dos trabalhos em PDF de todos os Simpósios Nacionais da Anpuh entre 1963 até 2017, disponíveis atualmente na site da associação, que podem ser acessados aqui. Escrito em Python 3.8, o script utiliza as seguintes bibliotecas e módulos urllib.requests: módulo do Python para acessar urls. Saiba mais. os: módulo do Python que permite manipular funções do sistema operacional. Saiba mais. bs4: Beautiful Soup é uma biblioteca Python para extrair dados de arquivos HTML e XML. re: Regular Expressions é um módulo do Python para operar com expressões regulares. pandas: Pandas é uma biblioteca escrita em Python para manipulação e análise de dados. wget: Wget é uma biblioteca escrita em Python para realizar downloads. O script tem o seguinte funcionamento quando executado: Cria pasta para salvar os PDFs, após verificar se a mesma não existe no local: Anais Anpuh&gt; pdf utilizando módulo os. Acessa a URL dos Anais com a biblioteca urllib e realiza a análise do HTML da mesma com a biblioteca BeautifulSoup; Cria uma lista de eventos a partir da página principal; Acessa as páginas de cada evento contidas na lista criada anteriormente através de uma iteração; Em cada item da lista de eventos, o script busca todos os papers da primeira página e cria uma nova lista. Nessa lista de papers de uma dada página o script realizará as seguintes ações: encontrar as informações de cada paper; inclui essas informações em uma lista (que depois gerará um CSV com os dados); busca se há pdf disponível e se ele não é repetido faz download do PDF Após realizar essas ações para todos os itens de uma página, busca a próxima página de papers do evento, se não houver, passa para o próximo evento e repete as ações em um loop até o último evento disponível. 4.2.2 Dados O script retorna para o usuário todos os pdfs disponíveis em todas as páginas de todos os Simpósios Nacionais da Anpuh desde 1963 até 2017. São criadas pastas com o número de cada evento para o armazenamento dos arquivos em PDF. É importante notar que muitos papers não estão com pdf disponível no site, assim como nas edições mais antigas encontramos arquivos que contém vários papers num único PDF. O script também gera um arquivo CSV (comma-separated values) contendo os seguintes valores para cada paper: Autor(es)/Instituições,Título, Tipo, Evento, Ano, Link do Arquivo. Esse arquivo pode ser aberto como uma planilha e trabalhado em banco de dados. 4.2.3 Resumos dos trabalhos da ANPUH Clique aqui para acessar o repositório no Github. Raspador dos resumos dos Simpósios Nacionais de História da Associação Nacional de História - Anpuh. O programa raspa todos os resumos dos SNH 27, 28, 29 e 30, respectivamente dos anos de 2013, 2015, 2017 e 2019 Escrito em Python 3.8, o script utiliza as seguintes bibliotecas e módulos urllib.requests: módulo do Python que ajuda a acessar urls. Saiba mais. bs4: Beautiful Soup é uma biblioteca Python para extrair dados de arquivos HTML e XML. pandas: Pandas é uma biblioteca escrita em Python para manipulação e análise de dados. O script tem o seguinte funcionamento quando executado: Pergunta ao usuário que ano pretende raspar e se deseja incluir um novo ano à lista. Após a criação da lista com os anos escolhidos pelo usuário, o script acessa cada uma das páginas com as listas dos STs nos sites de cada evento; Acessa cada ST, encontra os dados de todos os resumos e passa para o ST seguinte; Após terminar um ST, passa para o próximo evento e executa as mesmas função; Todos os dados são inseridos em um DataFrame em Pandas e ao final são salvos no formato CSV. 4.2.4 Dados O script retorna para o usuário um CSV (comma-separated values) com os dados de todos os trabalhos aceitos nos Simpósio Temáticos dos SNH 27, 28, 29 e 30. O CSV contém as seguintes variáveis para cada resumo: Ano, Evento, Cidade, ST, Coordenadores, Autor(es)/Instituições, Título, Resumo Esse arquivo pode ser aberto como uma planilha e trabalhado em banco de dados. Anpuh-Quem somos "],["anpocs.html", "5 ANPOCS 5.1 O que é a ANPOCS? 5.2 Script de raspagem 5.3 Dados 5.4 Em breve", " 5 ANPOCS 5.1 O que é a ANPOCS? A Associação Nacional de Pós-Graduação e Pesquisa em Ciências Sociais (ANPOCS) é uma entidade de direito privado sem fins lucrativos que reúne centenas de centros de pós-graduação e de pesquisa em antropologia, ciência política, relações internacionais e sociologia de todo o Brasil. Ela é formada, portanto, por instituições, em vez de pesquisadores individuais. A associação organiza os Encontros Anuais da ANPOCS, que consistem em congressos cujo número médio de participantes é de 1500 pesquisadores. Esses encontros estão entre os fóruns mais relevantes para as ciências sociais no Brasil. Diante disso, desenvolvemos o anpocs-scraper  disponível aqui , um raspador que permite coletar de forma automatizada os dados dos resumos dos trabalhos apresentados nos encontros de 2019, 2020 e, futuramente, 2021. O raspador expressa mais uma iniciativa que busca contribuir para uma ciência aberta e transparente, facilitando o acesso aos dados dos congressos e contribuindo para a preservação da memória das ciências sociais brasileiras. 5.2 Script de raspagem 5.2.1 anpocs-scraper O anpocs-scraper é um raspador dos dados dos Encontros Anuais da ANPOCS escrito em Python. Atualmente o código permite coletar: os dados de todos os resumos dos trabalhos apresentados em GTs e SPGs do 44º Encontro Anual da ANPOCS os dados de todos os resumos dos trabalhos apresentados em STs e SPGs do 43º Encontro Anual da ANPOCS 5.2.2 Instalação e modo de uso Para instalar o raspador basta clonar o repositório, que se encontra aqui, e instalar suas dependências: git clone https://github.com/vmussa/anpocs-scraper cd anpocs-scraper python -m venv .venv &amp;&amp; source .venv/bin/activate pip install -r requirements.txt Para rodar o raspador, continue no repositório clonado e execute o código main.py com o Python: python src/main.py Atenção  para realizar esse procedimento, você precisa instalar o Google Chrome e o ChromeDriver: Clique aqui para ler um tutorial sobre como instalar o ChromeDriver. 5.3 Dados O programa exporta, para cada edição do congresso, uma tabela no formato CSV com as seguintes informações de cada trabalho apresentado: autores, titulo, resumo, sessao, id_evento A imagem abaixo ilustra o formato de uma das tabelas: Caso você não queira rodar o raspador, mas precise dos dados, você pode obter a versão atual do conjunto de dados exportado pelo programa aqui. 5.4 Em breve Futuramente o raspador abarcará todos os GTs e SPGs do encontro 45, cujos resumos dos trabalhos estarão disponíveis aqui. Além disso, ele contará com um módulo de limpeza dos dados que fará o pré-processamento para a análise qualitativa e/ou computacional. Por fim, disponibilizaremos, também, todos os PDFs de artigos enviados para todas as edições dos Encontros Anuais da ANPOCS. "],["compós.html", "6 COMPÓS 6.1 O que é a COMPÓS? 6.2 Script de raspagem 6.3 Dados", " 6 COMPÓS 6.1 O que é a COMPÓS? A COMPÓS - Associação Nacional dos Programas de Pós-Graduação em Comunicação - foi fundada em 16 junho de 1991, em Belo Horizonte, com o apoio da Capes e do CNPq, a partir da iniciativa de alguns pesquisadores e representantes dos seguintes cursos de Pós-Graduação: PUC-SP, UFBA, UFRJ, UnB, UNICAMP, UMESP. É uma sociedade civil, sem fins lucrativos, congregando como associados os Programas de Pós-Graduação em Comunicação em nível de Mestrado e/ou Doutorado de instituições de ensino superior públicas e privadas no Brasil. A COMPÓS tem como objetivos principais o fortalecimento e qualificação crescentes da Pós-Graduação em Comunicação no país; a integração e intercâmbio entre os Programas existentes, bem como o apoio à implantação de novos Programas; o diálogo com instituições afins nacionais e internacionais; o estímulo à participação da comunidade acadêmica em Comunicação nas políticas do país para a área, defendendo o aperfeiçoamento profissional e o desenvolvimento teórico, cultural, científico e tecnológico no campo da Comunicação. Como espaço de intercâmbio acadêmico entre os pesquisadores dos vários Programas, a COMPÓS tem como fórum privilegiado os Encontros Anuais, estruturados sob a forma de Grupos de Trabalhos (GTs), onde são apresentados e debatidos estudos que buscam refletir sobre o avanço científico, tecnológico e cultural no campo da comunicação Diante disso, desenvolvemos o Anais-COMPOS-scraper  disponível aqui  que realiza o download automatizado de todos os papers em pdf dos Encontros da COMPÓS entre 2000 até 2020 (disponíveis atualmente na site). Além disso, o script também gera um arquivo CSV (comma-separated values) contendo as seguintes informações para cada paper: Ano, Edição, Nome do GT, Título, Autores, e Link do Arquivo. Esse arquivo pode ser aberto como uma planilha e trabalhado em banco de dados. O raspador expressa mais uma iniciativa que busca contribuir para uma ciência aberta e transparente, facilitando o acesso aos dados dos congressos e contribuindo para a preservação da memória das ciências sociais brasileiras. 6.2 Script de raspagem 6.2.1 R e RStudio O R e RStudio são gratuitos e possuem versões para Windows, Mac e Linux. A instalação é bastante fácil e em geral você apenas tem que seguir as instruções da tela. Para instalar o R, baixe a versão adequada para seu computador em: https://cloud.r-project.org/ Para instalar o RStudio, baixe a versão adequada para seu computador em: https://www.rstudio.com/products/rstudio/download/ Além disso, para ter um ambiente completo de desenvolvimento no R, recomendamos, adicionalmente, instalar:  MikTex (para Windows: http://miktex.org/download ou MacTex (para Mac: https://tug.org/mactex/downloading.html para relatórios em latex.  RTools (para Windows: https://cran.r-project.org/bin/windows/Rtools/ ou Xcode com command line tools (para Mac na AppStore do Mac), para criar pacotes, usar C++ com R entre outras coisas Após a instalação, vc pode executar o arquivo compos.R que está na pasta R direto do RStudio. 6.2.2 Bibliotecas e módulos Vocêr vai precisar instalar as seguintes bibliotecas: RSelenium tidyverse rvest 6.2.3 Chromedriver Instruções sobre como instalar o Chromedriver no Windows 10 : Instruções sobre como instalar o Chromedriver no Ubuntu : 6.3 Dados O programa exporta, para cada edição do congresso, uma tabela no formato CSV com as seguintes informações de cada trabalho apresentado: Ano, Edição, Nome do GT, Título, Autores, Links. A imagem abaixo ilustra o formato de uma das tabelas: Se preferir baixar a base dos PDFs sem usar o código clique aqui. (OBS:a nomeação ainda contém alguns pequenos erros que serão corrigidos em breve) Se preferir baixar a planilha sem usar o código clique aqui. "],["sobre-os-autores.html", "7 Sobre os autores 7.1 Leonardo F. Nascimento 7.2 Eric Brasil (IHLM/UNILAB) 7.3 Vítor Mussa (DTA/PPGSA/UFRJ e LABHDUFBA/UFBA) 7.4 Tarssio Barreto (LABHDUFBA) 7.5 Daniel Mendes (PATHS/UFRJ) 7.6 Gabriel Andrade (LABHDUFBA/UFBA) 7.7 Ajude o projeto!", " 7 Sobre os autores body { text-align: justify} 7.1 Leonardo F. Nascimento Sou formado em Química pelo Instituto Federal de Educação, Ciência e Tecnologia da Bahia  IFBA (1997), graduado em Psicologia pela Universidade Federal da Bahia  UFBA (2002), mestre em Sociologia pela Universidade de São Paulo  USP (2007) e doutor em Sociologia pelo Instituto de Estudos Sociais e Políticos  IESP/UERJ (2013). Entre 2010 e 2011, realizei estágio doutoral na École des Hautes Études en Sciences Sociales (EHESS). Nos últimos anos estou totalmente dedicado ao estudo das novas tecnologias aplicadas à pesquisa e análise de dados em Ciências Sociais, especialmente com uso de CAQDAS (Computer Assisted/Aided Qualitative Data Analysis). Eu sou professor do Bacharelado Interdisciplinar (BI) em Ciência, Tecnologia e Inovação da UFBA e membro permanente do Programa de Pós-Graduação em Ciências Sociais da UFBA (PPGCS/UFBA). Eu desenvolvo pesquisas sobre sociologia digital, mineração de dados, ciência social computacional e análise de mídia. Em 2018 eu ajudei a criar o Laboratório de Humanidades Digitais da UFBA, uma convergência de pesquisadores e interesses de pesquisa em torno dos temas da ciência social computacional, humanidades e métodos digitais. Currículos e redes acadêmicas Webpage - Lattes - LinkedIn - Twitter - Orcid - ResearchGate 7.2 Eric Brasil (IHLM/UNILAB) Professor do curso de licenciatura em História e professor do Bacharelado Interdisciplinar em Humanidades no Instituto de Humanidades e Letras da Universidade da Integração Internacional da Lusofonia Afro-brasileira (IHL-UNILAB), campus dos Malês, Bahia. Doutor (2016) e Mestre (2011) pelo Programa de Pós-Graduação em História Social da Universidade Federal Fluminense. Autor do livro A Corte em Festa: experiências negras em carnavais do Rio de Janeiro (1879-1888) (Editora Prismas, 2016). Vencedor do primeiro lugar no Concurso de Monografias Silvio Romero de 2011 e do segundo lugar em 2020, promovido pelo Centro Nacional de Folclore e Cultura Popular.Foi professor de ensino fundamental, médio e pré-vestibular no Rio de Janeiro entre 2007 e 2017. Pesquisador do Laboratório de Humanidades Digitais da UFBA. Membro do GT Nacional Emancipações e Pós-Abolição da Anpuh. Tem experiência na área de História Social da Cultura, Humanidades e História Digital, Abolição da escravidão e o Pós-Abolição no Brasil e no Caribe, atuando principalmente nos seguintes temas: Carnaval, Cidadania, História Transnacional, Diáspora Africana, História das Afro-Américas, Hemerotecas e arquivos digitais, métodos digitais de pesquisa, linguagem de programação para a pesquisa em História, web scraping. Currículos e redes acadêmicas Webpage - Lattes - Orcid - ResearchGate - Academia.edu 7.3 Vítor Mussa (DTA/PPGSA/UFRJ e LABHDUFBA/UFBA) Mestrando do Programa de Pós-Graduação em Sociologia e Antropologia (PPGSA) da Universidade Federal do Rio de Janeiro (UFRJ). É membro do grupo de pesquisa Desenvolvimento, Trabalho e Ambiente (DTA-UFRJ) e do Laboratório de Humanidades Digitais da Universidade Federal da Bahia (LABHD-UFBA). Currículos e redes acadêmicas Webpage - Lattes - ResearchGate - LinkedIn - Twitter 7.4 Tarssio Barreto (LABHDUFBA) Co-fundador da Bit Analytics, doutorando do Programa de Pós Graduação em Engenharia Industrial(PEI/UFBA) com foco na área de técnicas de agrupamentos voltadas para Machine Learning. Mestre em Meio Ambiente Águas e Saneamento - UFBA (2018) e graduado em Engenharia Sanitária e Ambiental pela UFBA (2015). Área de atuação: Machine Learning; modelagem probabilística e simulação e programação avançada em R.#RStats Lattes - LinkedIn - Twitter 7.5 Daniel Mendes (PATHS/UFRJ) Graduando no curso de Bacharelado em Ciências Sociais do Instituto de Filosofia e Ciências Sociais (IFCS) da Universidade Federal do Rio de Janeiro (UFRJ). É membro do grupo de pesquisa Núcleo de Pesquisa em Estratificação e Trajetórias Sociais (PATHS). Currículos e redes acadêmicas Lattes - LinkedIn - Twitter 7.6 Gabriel Andrade (LABHDUFBA/UFBA) Graduando no curso de Bacharelado em Engenharia de Computação da Escola Politécnica da Universidade Federal da Bahia (UFBA) É desenvolvedor de software e membro do Laboratório de Humanidades Digitais da Universidade Federal da Bahia (LABHDUFBA). Currículos e redes acadêmicas Webpage - Lattes - LinkedIn - Twitter 7.7 Ajude o projeto! bc1qmug7kcrw3kxklca7chy7c344d62gtc8fhqwnkw "]]
